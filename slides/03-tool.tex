\input{preamble}

\title{The tool: properties and flaws}
\author{Eivind Fonn}
\institute{SINTEF ICT / NTNU}
\date{December 2015}
\maketitle

\begin{frame}
  \frametitle{The tool}
  \begin{itemize}
    \item We here view the computer as a \emph{tool}---a means to reach a goal.
    \item Not a course in computer science, but a course in using this tool.
    \item No sane person would ever use the hammer without getting proper instructions!
  \end{itemize}
  \begin{center}
    \includegraphics[width=5cm]{wrong-tool-pizza} \\
    Image shamelessly stolen from the interwebs.
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Single processor systems: RISC architecture}
  \begin{center}
    \scalebox{0.8}{
      \input{\figs/single-proc}
    }
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Single processor systems: memory hierarchy}
  \begin{center}
    \scalebox{0.8}{
      \input{\figs/memory-hierarchy}
    }
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Single processor systems: memory hierarchy}

  Typical memory access times for the MIPS R14000 processor. The numbers
  represent number of clock cycles.
  \begin{center}
    \input{\data/memory-times}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Fixed point numbers}
  \begin{itemize}
  \item How to encode decimal numbers in the binary system?
  \item First alternative: Use $x$ bits to represent the stuff before the
    comma, and $y$ bits to represent the stuff after the comma, where $x+y = w$
    for a $w$ bit representation. This is called a fixed point representation.
  \item Problem: We get a fixed range of numbers we can represent; i.e.
    $2^x + 1 - 2^{-y}$ is the largest.
  \item Called fixed point since the point (the comma) is fixed. The
    \emph{absolute} accuracy is constant.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Floating point numbers}
  \begin{itemize}
  \item A better idea is to let the comma position ``float''.
  \item Floating-point numbers have constant \emph{relative} accuracy.
  \item Allows us to represent a much larger range of numbers.
  \end{itemize}
  \begin{center}
    \input{\figs/float}
  \end{center}
  \begin{itemize}
    \item[S] The sign bit (1 bit).
    \item[E] The exponent
    \item[F] The mantissa
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Floating point numbers}
  Thus;
  \[
    V = (-1)^S \cdot 2^{E-B}\cdot M
  \]
  where the mantissa $M$ is defined as
  \[
    M = \underbrace{1}_{\times2^0} .
    \overbrace{\underbrace{b_1}_{\times2^{-1}}
      \underbrace{b_2}_{\times2^{-2}}\ldots}^{F}
  \]
  for normalized numbers (most common). Here $B$ is the \emph{bias}. Common
  precisions:
  \begin{center}
    \input{\data/precision}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Floating point numbers}
  \begin{itemize}
  \item The \emph{bias} allows us to give a bias to large or small numbers
    (large or small exponents).
  \item Since we use a finite representation, we have a finite precision.
  \item Smallest and largest numbers:
    \begin{align*}
      V_\text{min} &= 1\cdot 2^{1-127}\cdot 1 = 2^{-126} = 1.17\ldots 10^{-38} \\
      V_\text{max} &= 1\cdot 2^{254-127}\cdot 2 = 3.40\ldots 10^{38}.
    \end{align*}
  \item More important: The smallest \emph{relative} difference between numbers
    we can represent
    \[
      2^{-23} = 1.19....10^{-7}.
    \]
    i.e.~under perfect circumstances we have about 7 digits of accuracy.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Floating point operations}
  \begin{itemize}
    \item A very useful estimate for the size of a scientific code is the total
      number of floating point operations performed.
    \item Floating point operations: $+$, $-$, $\times$, $/$
    \item A floating point operation is called a Flop.
    \item It is tempting to use Flops for the plural, but that is \emph{not} the
      norm. Rather, Flops is used about Flop per second, which is a very useful
      performance metric.
    \item $\SI{3}{\flop}$ or $\SI{1}{\flop}$ (both Flop per second).
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Floating point limitations}
  \begin{itemize}
  \item We have a finite precision in our number representation. This leads to
    some issues.
  \item Subtraction: $1.2345\cdot 10^4$ minus $1.2344\cdot 10^4$.
    \begin{align*}
      & 1.2345 \cdot 10^4 - 1.2344 \cdot 10^4 \\
      &= (1.2345 - 1.2344) \cdot 10^4 \\
      &= \textcolor{red}{0.0001} \cdot 10^4 \\
      &= 1.0000 \cdot 10^0
    \end{align*}
    We have only a single digit of accuracy! This is called \emph{cancellation}.
  \item Example where this matters: Approximations of derivatives
    \[
      \frac{\text{d}f}{\text{d}x} \approx \frac{1}{h}\left( f(x+h) - f(x) \right)
    \]
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Floating point limitations}
  \begin{itemize}
  \item Additition: $1.2345\cdot 10^4$ plus $1.0000\cdot 10^0$.
    \begin{align*}
      & 1.2345 \cdot 10^4 + 1.0000 \cdot 10^0 \\
      &= (1.2345 + 0.0001) \cdot 10^4 \\
      &= 1.2346 \cdot 10^4
    \end{align*}
    OK!
  \item Additition: $1.2345\cdot 10^4$ plus $1.0000\cdot 10^{-1}$.
    \begin{align*}
      & 1.2345 \cdot 10^4 + 1.0000 \cdot 10^0 \\
      &= (1.2345 + 0.0000) \cdot 10^4 \\
      &= 1.2345 \cdot 10^4
    \end{align*}
    Adding the small number has no effect.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Pipelining vectorization}
  Consider the fused scalar multiplication and vector addition:
  \begin{align*}
    \bm c = \bm a + \gamma \bm b
  \end{align*}
  We can write this operation as the loop
  \begin{center}
    \begin{tabular}{c}
\begin{lstlisting}[style=fortran]
for i=1,n
  c(i) = a(i) + gamma * b(i)
end
\end{lstlisting}
    \end{tabular}
  \end{center}
  A single iteration is performed in a certain number of stages. These stages
  can be viewed as a pipeline.
\end{frame}

\begin{frame}
  \frametitle{Superscalar operations}
  \begin{align*}
    \bm c = \bm a + \gamma \bm b
  \end{align*}

  \begin{center}
    \input{\figs/superscalar} \\
    Fused multiply and add (FMADD)
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Vectorization}
  Modern processors contain vector (SIMD, Single Instruction Multiple Data)
  units. This allows to apply the same operation to multiple data in parallel.
  In modern Intel (Sandy Bridge, Haswell) chips, the vector units are also
  superscalar (both AVX and SSE).

  Three ways to enable SIMD usage in your programs:
  \begin{itemize}
  \item autovectorizing compilers (ICC),
  \item hand-written assembly,
  \item intrinsics (built-in functions)
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Direct mapped cache}
  \begin{center}
    \input{\figs/direct-mapped-cache}
  \end{center}
  \[
    \text{Memory address} =
    \underbrace{b_1 \ldots b_k}_{\text{tag bits}}
    \underbrace{b_{k+1} \ldots b_{N}}_{\text{cache address}}.
  \]
\end{frame}

\begin{frame}[fragile]
  \frametitle{Direct mapped cache}
  \begin{center}
    \begin{tabular}{c}
\begin{lstlisting}[style=fortran]
for i=1,n
  c(i) = a(i) + b(i)
end
\end{lstlisting}
    \end{tabular}
  \end{center}

  If the vectors are precisely the same length as a cache line, we get cache
  trashing.

  \begin{center}
    \input{\figs/cache-trashing}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Adjacent memory layout}
  \begin{center}
    \input{\figs/adjacent-memory} \\
    Interleaving the vectors in memory avoids cache trashing.
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Other cache strategies}
  \begin{itemize}
  \item Fully associative cache: \emph{all} bits in a memory address are used as
    tag bits, none of them as cache address. All cache addresses are available.
  \item $n$-way associative cache: uses $m$ fewer bits for cache address than a
    direct mapped cache, thus freeing up a choice of $n = 2^m$ cache addresses
    for each chunk. This is a good compromise between direct mapped cache and
    fully associative cache
  \end{itemize}
\end{frame}

\input{postamble}
