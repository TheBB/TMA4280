\input{preamble}

\title{Message passing and MPI}
\author{Eivind Fonn}
\institute{SINTEF ICT / NTNU}
\date{December 2015}
\maketitle

\begin{frame}
  \frametitle{The message passing model}
  \begin{center}
    \input{\figs/message-passing}
  \end{center}
\end{frame}

\begin{frame}[fragile]
  \frametitle{MPI with C}
  \scalebox{0.5}{
    \lstinputlisting[style=c]{\code/mpi_hello/c/hello.c}
  }
\end{frame}

\begin{frame}[fragile]
  \frametitle{MPI with Fortran}
  \scalebox{0.5}{
    \lstinputlisting[style=fortran]{\code/mpi_hello/fortran/hello.f90}
  }
\end{frame}

\begin{frame}[fragile]
  \frametitle{Sample output with four processors}
  \begin{center}
    \begin{tabular}{c}
\begin{lstlisting}
Process 0: Hello, world!
Process 1: Hello, world!
Process 3: Hello, world!
Process 2: Hello, world!
\end{lstlisting}
    \end{tabular}
  \end{center}
  Note that ordering is not guaranteed.
\end{frame}

\begin{frame}
  \frametitle{MPI: Message Passing Interface}
  Advantages of the MPI message-passing model:
  \begin{itemize}
  \item standardization;
  \item portability;
  \item performance;
  \item expressiveness.
  \end{itemize}

  MPI is a library comprising about 125 functions or operations:
  \begin{itemize}
  \item \textcolor{red}{one-to-one} operations (or point-to-point communication);
  \item \textcolor{red}{one-to-all} operations;
  \item \textcolor{red}{all-to-one} operations;
  \item \textcolor{red}{all-to-all} operations.
  \end{itemize}
  The last three types are referred to as {\em collective} operations.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Header file}
  The header file is required. For C:
\begin{lstlisting}[style=c]
#include "mpi.h"
\end{lstlisting}

  And for Fortran:
\begin{lstlisting}[style=fortran]
include 'mpif.h'
\end{lstlisting}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Calling functions}
  For C:
\begin{lstlisting}[style=c,morekeywords={err}]
err = MPI_Xxxxx(parameters, ...);
\end{lstlisting}
  Or ignoring the error code:
\begin{lstlisting}[style=c,morekeywords={err}]
MPI_Xxxxx(parameters, ...);
\end{lstlisting}

  For Fortran, everything is a subroutine:
\begin{lstlisting}[style=fortran,morekeywords={err}]
call MPI_XXXXX(parameters, ..., err)
\end{lstlisting}
\end{frame}

\begin{frame}
  \frametitle{Six essential functions}
  \begin{center}
    \bgroup\def\arraystretch{1.2}
    \addtolength{\tabcolsep}{0.5cm}
    \begin{tabular}{ll}
      \hline
      C & Fortran  \\
      \hhline{==}
      \texttt{MPI\_Init} & \texttt{call MPI\_Init} \\
      \texttt{MPI\_Comm\_size} & \texttt{call MPI\_Comm\_size} \\
      \texttt{MPI\_Comm\_rank} & \texttt{call MPI\_Comm\_rank} \\
      \texttt{MPI\_Send} & \texttt{call MPI\_Send} \\
      \texttt{MPI\_Recv} & \texttt{call MPI\_Recv} \\
      \texttt{MPI\_Finalize} & \texttt{call MPI\_Finalize} \\
      \hline
    \end{tabular}
    \addtolength{\tabcolsep}{-0.5cm}
    \egroup
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Message = data + envelope}
  \texttt{\textcolor{red}{MPI\_Send}($\underbrace{buffer, count, datatype}_{\textcolor{blue}{data}}$,
    $\underbrace{dest, tag, comm}_{\textcolor{blue}{envelope}}$); } \\
  \texttt{\textcolor{red}{MPI\_Recv}($\underbrace{buffer, count, datatype}_{\textcolor{blue}{data}}$,
    $\underbrace{source, tag, comm,\&status}_{\textcolor{blue}{envelope}}$); }

  Examples of predefined data types (C):
  \begin{itemize}
  \item \texttt{MPI\_CHAR}
  \item \texttt{MPI\_INT}
  \item \texttt{MPI\_FLOAT}
  \item \texttt{MPI\_DOUBLE}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Point-to-point communication}
  \texttt{MPI\_Send} and \texttt{MPI\_Recv} are \emph{blocking}. Deadlock
  example:
  \begin{center}
    \begin{tabular}{c}
\begin{lstlisting}[style=c,morekeywords={MPI_Recv,MPI_Send}]
if (rank == 0) {
  MPI_Recv(...);
  MPI_Send(...);
}
else if (rank == 1) {
  MPI_Recv(...);
  MPI_Send(...);
}
\end{lstlisting}
    \end{tabular}
  \end{center}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Point-to-point communication}
  How to avoid deadlock.
  \begin{center}
    \begin{tabular}{c}
\begin{lstlisting}[style=c,morekeywords={MPI_Recv,MPI_Send}]
if (rank == 0) {
  MPI_Send(...);
  MPI_Recv(...);
}
else if (rank == 1) {
  MPI_Recv(...);
  MPI_Send(...);
}
\end{lstlisting}
    \end{tabular}
  \end{center}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Collective operations}
  In general,
  \begin{itemize}
  \item Involves all of the processes in a group, and
  \item are more efficient and less tedious to use compared to point-to-point
    communication.
  \end{itemize}
  An example (synchronization between processes):
  \begin{center}
    \begin{tabular}{c}
\begin{lstlisting}[style=c,morekeywords={MPI_Barrier}]
MPI_Barrier(comm);
\end{lstlisting}
    \end{tabular}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Collective operations}
  \begin{center}
    \begin{tikzpicture}[scale=0.6]
      \foreach \i in {0,...,4} {
        \foreach \j in {0,9} {
          \foreach \k in {0,6} {
            \draw[darkblue, thick] (\i+\j,0+\k) -- (\i+\j,4+\k);
            \draw[darkblue, thick] (0+\j,\i+\k) -- (4+\j,\i+\k);
          }
        }
      }
      \node at (0.5,9.5) {$A_0$};
      \node at (9.5,9.5) {$A_0$};
      \node at (9.5,8.5) {$A_0$};
      \node at (9.5,7.5) {$A_0$};
      \node at (9.5,6.5) {$A_0$};

      \node at (0.5,3.5) {$A_0$};
      \node at (0.5,2.5) {$A_1$};
      \node at (0.5,1.5) {$A_2$};
      \node at (0.5,0.5) {$A_3$};
      \node at (9.5,3.5) {$A_0$};
      \node at (10.5,3.5) {$A_1$};
      \node at (11.5,3.5) {$A_2$};
      \node at (12.5,3.5) {$A_3$};
      \node[color=black!30] at (9.5,2.5) {$A_1$};
      \node[color=black!30] at (9.5,1.5) {$A_2$};
      \node[color=black!30] at (9.5,0.5) {$A_3$};

      \draw[red, very thick, ->] (4.5,8) -- (8.5,8);
      \draw[red, very thick, ->] (4.5,2) -- (8.5,2);
      \node[anchor=north] at (6.5,8) {\texttt{MPI\_Bcast}};
      \node[anchor=south] at (6.5,8) {\scriptsize One-to-all broadcast};
      \node[anchor=north] at (6.5,2) {\texttt{MPI\_Gather}};
      \node[anchor=south] at (6.5,2) {\scriptsize All-to-one gather};

      \node[anchor=west, color=darkblue] (data) at (0,10.5) {\scriptsize Data};
      \node[anchor=west, color=darkblue, rotate=-90] (procs) at (-0.5,10) {\scriptsize Processes};
      \draw[darkblue, ->] (data.east) -- ($ (data.east) + (1.5,0) $);
      \draw[darkblue, ->] (procs.east) -- ($ (procs.east) - (0,1.5) $);
    \end{tikzpicture}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Global reduction}
  \begin{center}
    \begin{tikzpicture}[scale=0.6]
      \foreach \i in {0,4,8,12} {
        \foreach \j in {0,4,8} {
          \draw[darkblue, thick] (\i,\j) rectangle (\i+2,\j+1);
          \draw[darkblue, thin] (\i+1,\j) -- (\i+1,\j+1);
        }
      }

      \node[anchor=south] at (7,9) {Processes with initial data};
      \node[anchor=north] at (1,8) {\scriptsize $p=0$};
      \node[anchor=north] at (5,8) {\scriptsize $p=1$};
      \node[anchor=north] at (9,8) {\scriptsize $p=2$};
      \node[anchor=north] at (13,8) {\scriptsize $p=3$};
      \node at (0.5,8.5) {$2$}; \node at (1.5,8.5) {$4$};
      \node at (4.5,8.5) {$5$}; \node at (5.5,8.5) {$7$};
      \node at (8.5,8.5) {$0$}; \node at (9.5,8.5) {$3$};
      \node at (12.5,8.5) {$6$}; \node at (13.5,8.5) {$2$};

      \node[anchor=south] at (7,5) {After \texttt{MPI\_Reduce(..., MPI\_MIN, 0, ...)}};
      \node at (0.5,4.5) {$0$}; \node at (1.5,4.5) {$2$};
      \node at (4.5,4.5) {$-$}; \node at (5.5,4.5) {$-$};
      \node at (8.5,4.5) {$-$}; \node at (9.5,4.5) {$-$};
      \node at (12.5,4.5) {$-$}; \node at (13.5,4.5) {$-$};

      \node[anchor=south] at (7,1) {After \texttt{MPI\_Allreduce(..., MPI\_MIN, ...)}};
      \node at (0.5,0.5) {$0$}; \node at (1.5,0.5) {$2$};
      \node at (4.5,0.5) {$0$}; \node at (5.5,0.5) {$2$};
      \node at (8.5,0.5) {$0$}; \node at (9.5,0.5) {$2$};
      \node at (12.5,0.5) {$0$}; \node at (13.5,0.5) {$2$};
    \end{tikzpicture}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Global reduction}
  \texttt{\textcolor{red}{MPI\_Reduce}($\underbrace{sbuf, rbuf, count, datatype}_{\textcolor{blue}{data}}$,
    $\underbrace{op, root, comm}_{\textcolor{blue}{envelope}}$); } \\
  \texttt{\textcolor{red}{MPI\_Allreduce}($\underbrace{sbuf, rbuf, count, datatype}_{\textcolor{blue}{data}}$,
    $\underbrace{op, comm}_{\textcolor{blue}{envelope}}$); }

  Examples of predefined operations (C):
  \begin{itemize}
  \item \texttt{MPI\_SUM}
  \item \texttt{MPI\_PROD}
  \item \texttt{MPI\_MIN}
  \item \texttt{MPI\_MAX}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{A ``problem'': defining $\pi$ through integration}
  \begin{align*}
    & \int_0^1\frac{1}{1+x^2} \dif{x} \\
    &= [\arctan(x)]_0^1 \\
    &= \arctan(1) - \arctan(0) = \frac{\pi}{4}
  \end{align*}
  Hence,
  \begin{align*}
    \pi = \int_0^1\frac{4}{1+x^2} \dif{x}
  \end{align*}
\end{frame}

\begin{frame}
  \frametitle{Numerical integration}
  \begin{center}
    \begin{tikzpicture}[scale=0.7]
      \begin{axis}[
        xmin=0,
        xmax=1.1,
        ymin=0,
        ymax=5,
        axis lines=middle,
        xlabel={$x$},
        ylabel={$f(x)$},
        xtick={0.001,0.4,1},
        xticklabels={$0$, $x_i$, $1$},
        ]
        \draw[darkblue, fill=cadet] (axis cs:0.36,0) rectangle (axis cs:0.44,3.4482);
        \addplot[darkblue, thick, domain=0:1, samples=100]{4/(1+x^2)};
      \end{axis}
    \end{tikzpicture}
  \end{center}
  \[
    A_i = \left ( \frac{4}{1+x_i^2} \right ) \cdot h, \qquad
    \text{with} \quad x_i = \left (i+\frac{1}{2} \right )\cdot h
  \]
  where $i=0,\ldots,n-1$, and $h=1/n$.
\end{frame}

\begin{frame}
  \frametitle{Numerical integration}
  \begin{align*}
    \pi = \int_0^4 \frac{4}{1 + x^2} \dif{x} \approx
    h \sum_{i=0}^{n-1} \frac{4}{1 + x_i^2} = \pi_n
  \end{align*}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Calculating pi with MPI in C}
  \begin{center}
    \begin{tabular}{c}
      \scalebox{0.65}{
      \lstinputlisting
      [style=c, firstline=1, lastline=22, morekeywords={
      MPI_Init, MPI_Comm_size, MPI_Comm_rank, MPI_Wtime}]{\code/pi/pi.c}
      }
    \end{tabular}
  \end{center}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Calculating pi with MPI in C (cntd.)}
  \begin{center}
    \begin{tabular}{c}
      \scalebox{0.65}{
      \lstinputlisting
      [style=c, firstline=24, lastline=45, morekeywords={
      MPI_Reduce, MPI_Finalize, MPI_Wtime}]{\code/pi/pi.c}
      }
    \end{tabular}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Things to consider}
  \begin{enumerate}
  \item Is the program correct, e.g., is the convergence rate as expected?
  \item Is the program load-balanced?
  \item Do we get the same value of $\pi_n$ for different values of $P$?
  \item Is the program scalable?
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Convergence test}
  \begin{center}
    \bgroup\def\arraystretch{1.2}
    \begin{tabular}{cc}
      \hline
      $n$ & $\text{error} = |\pi - \pi_n |$
      \\ \hhline{==} 10 & $8.33\cdot 10^{-4}$
      \\ \hline $10^2$ & $8.33\cdot 10^{-6}$
      \\ \hline $10^3$ & $8.33\cdot 10^{-8}$
      \\ \hline $10^4$ & $8.33\cdot 10^{-10}$
      \\ \hline $10^5$ & $8.37\cdot 10^{-12}$
      \\ \hline
    \end{tabular}
    \egroup
  \end{center}
  Hence, $|\pi - \pi_n | \sim {\cal O}(h^2)$ where $h = 1/n$.
\end{frame}

\begin{frame}
  \frametitle{Scalability: timing results on Vilje}
\end{frame}

\input{postamble}
