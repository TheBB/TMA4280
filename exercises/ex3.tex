\documentclass[onecolumn, oneside, a4paper, 11pt]{memoir}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% Paths
\newcommand{\figs}{../figs}
\newcommand{\data}{../data}

% Fonts
\usepackage{newpxtext,newpxmath}
\renewcommand*\sfdefault{cmss}

% References
\usepackage{hyperref}

% Units
\usepackage[detect-weight=true, binary-units=true]{siunitx}
\DeclareSIUnit\flop{Flops}

% Math
\let\openbox\undefined
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}

\theoremstyle{remark}
\newtheorem{ex}{Exercise}
\newtheorem*{sol}{Solution}

% Graphics
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\graphicspath{{../figs/}}

% Tikz
\usepackage{tikz}
\usetikzlibrary{positioning,shapes,arrows,calc,intersections}
\usepackage{pgfplots}
\usepgfplotslibrary{dateplot}
\pgfplotsset{compat=1.8}

% Colors
\definecolor{darkblue}{HTML}{00688B}
\definecolor{darkgreen}{HTML}{6E8B3D}
\definecolor{cadet}{HTML}{DAE1FF}
\definecolor{salmon}{HTML}{FFB08A}

% Listings
\usepackage{textcomp}
\usepackage{listings}
\lstset{
  keywordstyle=\bfseries\color{orange},
  stringstyle=\color{darkblue!80},
  commentstyle=\color{darkblue!80},
  showstringspaces=false,
  basicstyle=\ttfamily,
  upquote=true,
}
\lstdefinestyle{fortran}{
  language=Fortran,
  morekeywords={for},
  deletekeywords={status},
}
\lstdefinestyle{c}{
  language=C,
  morekeywords={include},
}
\lstdefinestyle{shell}{
  language=bash,
}

\begin{document}

\pagestyle{empty}

\begin{center}
  {\Huge \bfseries \scshape
    Introduction to \\[0.2\baselineskip] Supercomputing} \\[2\baselineskip]
  {\Large TMA4280 $\cdot$ Problem set 3} \\[2\baselineskip]
\end{center}

\begin{ex}
  Go to \url{http://top500.org}, a website cataloguing the top 500
  supercomputers in the world. Study the top 10, in particular their technical
  specifications. What is meant by the LINPACK benchmark performance?
\end{ex}
\begin{sol}
  Self study.
\end{sol}

\begin{ex}
  Note that some of this was not covered in detail in the lecture. Please have a
  look at the lecture notes.
  \begin{enumerate}
  \item What limits the scalability of a bus-based interconnect?
  \item How are the individual processors connected using a crossbar?
  \item How are the individual processors connected using a mesh?
  \item What is the difference between a shared-memory and a distributed
    memory architecture?
  \item What characterizes the memory access in an SMP?
  \item What is the difference between a NUMA and a ccNUMA architecture?
  \end{enumerate}
\end{ex}
\begin{ex} ~
  \begin{enumerate}
  \item The limitation of a bus-based system lies in the fact that the total
    bandwith is fixed, and given by the bus. We can easily add more processors,
    but they must share the same bus, so the total amount of bandwidth available
    to each processor \emph{on average} is reduced.

    Note that caches can reduce bandwidth demand, since if a value can be found
    in the cache there's no need to use the bus. This introduces the problem of
    cache consistency.
  \item In a crossbar configuration, there are multiple paths between each
    processor and memory unit. Here, the bandwith available to each processor
    remains constant as more processors are added. However, each added processor
    is associated with much higher cost than in the bus case.
  \item In a mesh topology, each processor is connected to the directly
    neighboring processors according to a fixed regular pattern, resembling a
    structured mesh in $d$ dimensions. Each processor in the interior is
    connected to $2d$ other processors, two for each dimension (negative and
    positive direction). A variation on this structure is a toroidal system in
    which the same holds for boundary processors.
  \item In a shared-memory system, all processors have access to all the
    available memory in the same address space. In a distributed memory system,
    each processor only has local memory access, and data stored in other units
    can only be retrieved with explicit message passing.
  \item In an SMP, the memory access time is (nearly) constant, independent of
    processor and memory unit.
  \item ccNUMA stands for \emph{cache-coherent} NUMA. A ccNUMA system has some
    form of mechanism for ensuring that the caches of each processor remain
    consistent with the others and the main memory units.
  \end{enumerate}
\end{ex}

\begin{ex}
  How many bytes are sent in each of the three messages listed below? Here given
  in C, but that's not important.

\begin{lstlisting}[style=c]
MPI_Send(buf1, 80, MPI_CHAR, dest, tag, MPI_COMM_WORLD);
MPI_Send(buf2, 1024, MPI_INT, dest, tag, MPI_COMM_WORLD);
MPI_Send(buf3, 1024, MPI_DOUBLE, dest, tag, MPI_COMM_WORLD);
\end{lstlisting}
\end{ex}
\begin{sol} ~
  \begin{enumerate}
  \item Each char uses one byte, so a message of 80 chars uses 80 bytes of
    memory.
  \item On most modern systems today, a C int is four bytes, so such a message
    would require $1024 \cdot 4 = 4096$ bytes of memory. Note that the C
    standard only requires an int to be at least two bytes.
  \item A double is eight bytes, so this message requires $1014 \cdot 8 = 8192$
    bytes of memory.
  \end{enumerate}
\end{sol}

\begin{ex}
  Is it true that a unique tag must be specified each time \texttt{MPI\_Recv} is
  called?
\end{ex}
\begin{sol}
  No, for example it is possible to receive messages with any tag by using
  \texttt{MPI\_ANY\_TAG} in \texttt{MPI\_Recv}.
\end{sol}

\begin{ex}
  Implement the program from the previous exercise using MPI. It should run on
  any number of processes.

  Hint: Partition the matrix in column strips.
\end{ex}
\begin{sol}In C:
  \lstinputlisting[style=c]{ex3.5.c}
\end{sol}

\end{document}
