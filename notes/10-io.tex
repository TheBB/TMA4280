\lstset{inputpath=code/mpiio/}

\chapter{Parallel I/O in MPI}

\section{Introduction}

As HPC applications grow larger and larger as more and more computing resources
are made available, so does the volume of data which needs to be handled, both
on the input and the output side of the application. The process of reading data
into a process or storing data from a process we hereby refer to as \emph{I/O}.
The input side is mostly a solved problem, in the sense that most operating
systems and filesystems added support for multiple processes reading from the
same file years ago.

\begin{figure}
  \centering
  \includegraphics[width=12cm]{Hard_drive-en}
  \caption{
    Illustration of the interior of a hard disk drive. The data is stored on
    platters. The data is stored magnetically on these platters by a magnetic
    device referred to as the \emph{head}. The same device is responsible for
    retrieving the data when a read operation is issued. Since we only have a
    single head, HDDs are serial by construction. Image taken from
    \url{http://en.wikipedia.org/wiki/Harddrive}.
  }
  \label{fig:hdd}
\end{figure}

Getting high performance, however, is more involved. For the last thirty years
or so, secondary storage has usually been hard disk drives (commonly abbrevated
as \emph{HDDs}); see \autoref{fig:hdd} for an illustration of the interior.
These are mechanical in nature, data is read/written from/to spinning platters
through a magnetic device referred to as the \emph{head}. Since a HDD only has a
single head, it can only perform a single read/write operation concurrently.
This means that even though multiple processes can initiate reads concurrently,
they will be performed in serial. Schematically this can be illustrated as in
\autoref{fig:manyone}. A HDD only performs at peak levels if data is read or
written in large sequential chunks, since searching for data incur large
penalties, as the head has to be repositioned. In a lot of HPC applications, the
access pattern for a single process can seem to be fairly random from the
operating system's point of view, which lead to the I/O being performed in an
inefficient manner. This is due to the fact that the data a particular process
needs is scattered in the file, leading to excessive seeks. Only when all the
I/O performed by the separate processes are considered as a whole, a sequential
pattern can be found. The application developer is often aware of this fact, and
this raises the need for an interface where this information can be given to the
system. Additional performance can be achieved if a file is stored across
multiple storage devices. This allows us to harness the aggregated bandwith of
the devices; see \autoref{fig:manymany} for an illustration.

\begin{figure}
  \begin{center}
    \includegraphics[width=12cm]{many-onedisk}
  \end{center}
  \caption{
    Illustration of I/O where several processes read from a single file. Since a
    HDD is serial in nature, only one operation can be performed concurrently.
  }
  \label{fig:manyone}
\end{figure}
\begin{figure}
  \begin{center}
    \includegraphics[width=12cm]{many-manydisk}
  \end{center}
  \caption{
    Illustration of I/O where we harness the aggregated bandwith of several
    devices. Here each process reads its data from a separate physical device,
    thus the I/O can be performed in parallel.
  }
  \label{fig:manymany}
\end{figure}

On the output side, things turn are interesting. For years HPC applications
solved their needs using one of two approaches. The first approach is often
referred to as \emph{post-mortem assembly}; each process dumps its data to a
separate file, and then custom-tailored code is used to read the data into the
next application in the computational chain, such as visualization software. If
implementing the support directly into the visualization software is not
feasible, a separate postprocessing application must be written. The second
approach, which is also often used, is to serialize the I/O. Here one process is
given the responsibility of writing the data to disc. The other processes then
simply send their data to the responsible process, in a sequential manner; see
\autoref{fig:serial}.

\begin{figure}
  \begin{center}
    \includegraphics[width=12cm]{serial}
  \end{center}
  \caption{
    Illustration of serialization of the I/O. All processes send their data to
    process 0, which is responsible for writing the data to secondary storage.
    Since a harddrive is serial in nature, the total time spent on the operation
    equals the sum of the time to write the individual data chunks.
  }
  \label{fig:serial}
\end{figure}

What is common to both of these approaches, is that they necessitate performing
(most of) the I/O in a serial manner. As the data volume grows, this occupies an
increasing amount of the total computational time. Eventually this may become a
large bottleneck for the parallel performance of your program. The first
approach also necessitates performing substantially more I/O than strictly
required, in particular all data is written twice; first to the separate files
and then in a stitched-together fashion as performed by the postprocessing
application. If the volume of data is substantial, this introduces another
severe bottleneck in your computational chain. The code involved in both of
these approaches is often intricate and prone to errors.

It is thus of great interest to be able to store all data in a single file, or
at most a few files, while avoiding serialization of the I/O both on the
application level, as well as on the physical level, by splitting the file
across multiple storage devices. Fortunately there are established libraries to
enable this. We here consider one particular example of such an interface,
namely the \emph{MPI-IO} interface. As the name implies, this was made part of
the official MPI standard back in 1998 with the introduction of the MPI 2.0
specification. However, while the library strive to yield highly portable code,
tuning to the underlying filesystem (how the data is stored on the hard disk
drive(s)) is unavoidable when it comes to obtaining high performance. One
example of such a file system is the \emph{general parallel filesystem}, GPFS
for short, developed by IBM during the last 10 years. This is in use on NOTUR
systems (\emph{Kongull}, \emph{Vilje}). Since such tuning is somewhat technical
and beyond the intention of this document, this will not be discussed in detail
in the following.

\section{Basic concepts of MPI-IO}

MPI-IO is the part of the official MPI standard which covers parallel I/O. It
offers a simple, natural interface for expressing parallelism in I/O. Informing
the system about the underlying distributed nature of an I/O call enables the
system to make good choices about how the I/O is performed on a lower level. We
first consider the simplest case: I/O that is sequential on each process.

\subsection{Sequential I/O}

We here consider sequential I/O, namely writing a contiguous slice of data from
a set of separate processes. The data storage pattern in this case is fairly
trivial, as illustrated in \autoref{fig:contigousstorage}. An example where such
a pattern could occur, is if we have partitioned a matrix in a strip fashion
across the processes; see \autoref{fig:strip} for an illustration.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=12cm]{slicedvector}
  \end{center}
  \caption{
    Illustration of an I/O operation where a contiguous piece of data is sliced
    into chunks. A single contiguous chunk of data is then written by each
    process. All the chunks are written to the same file.
  }
  \label{fig:contigousstorage}
\end{figure}

If you are familiar with standard POSIX I/O calls, the basic form of
the MPI-IO code should look very familiar. On each process we start by
opening a handle to the file we want to write to, in this case called
``datafile''.
\lstinputlisting[style=c]{open-handle.c}
A file handle is described by a variable of type \texttt{MPI\_File}. In addition
to which file we want to open, we also have to specify flags which the system
use to both control which accesses are allowed, as well as for directives which
help increase performance. Here we only specify flags of the former class, in
particular, we only want to write to the file (by specifying the flag
\texttt{MPI\_MODE\_WRONLY}) and we want the file to be created if it doesn't
already exist (by specifying the flag \texttt{MPI\_MODE\_CREATE}). The function
includes an additional parameter of type \texttt{MPI\_Info}. For now we pass the
builtin value \texttt{MPI\_INFO\_NULL} which can be used when we do not want to
pass any data in this field. We will briefly comment on this later.

With the file opened, we proceed with writing the data. In this example, our
data is an array of doubles named \texttt{vec}. The total global array length is
\texttt{N}, which means we have \texttt{N/size} elements per process (here
\texttt{size}, as usual, denotes the number of processes, while \texttt{rank}
denotes the process number). We first seek to the appropriate offset in the
file, then write the chunk of data.
\lstinputlisting[style=c]{separate-handle-write.c}

An alternative way of doing the same operation, is to use the \emph{explicit
offset} \texttt{MPI\_File\_Write\_at} function. This is mainly a matter of
convenience, i.e. it allows to do the same operation with less code, in
particular, we avoid the seek call;
\lstinputlisting[style=c]{explicit-offset-write.c}

Common to both of these alternatives is that they use individual I/O calls on
each process. If the underlying system is smart, it should be able to recognize
the access pattern. If we want to make sure that the system notices this fact,
we can use \emph{collective} calls. In particular, consider
\lstinputlisting[style=c]{collective-write.c}
Here we inform the system that we are performing a collective write among all
the processes. This allows the system to do additional coordination of how the
I/O is performed. However, it comes at a cost. In particular, all processes will
stall until the entire I/O operation is completed across all the processes,
while using individual handles allows the program flow to continue on each
process as soon as their share of the I/O has been performed.

Another convenience function in the context of collective calls is \emph{shared}
writes. In addition to containing info about the position on the individual
processes, a \texttt{MPI\_File} variable also contains info about a shared state
between all the processes. Here, with our simple data layout, this can be
exploited to make the code particularly simple. We simply have to do
\lstinputlisting[style=c]{shared-write.c}
This function uses the shared state in the file pointer. Upon the call, the writes
are performed in order according to process rank.

Finally we have to close our file. This is performed by doing
\lstinputlisting[style=c]{close.c}

\section{Non-sequential I/O: file views}

In the previous section we considered an extremely simple data access pattern.
Such a pattern is only the predominant one in simple codes. Usually the access
patterns in real applications are much more involved. Programming such access
patterns naively using many seeks and many small writes is certainly a
possibility, however hardly something we would recommend. In addition to being
highly error prone, it would also make it very hard for the system to get a
proper view of the collective I/O performed across the processes.

For purposes of easy presentation, we here again consider a simple data layout.
In particular, consider a vector split in a cyclic manner, see
\autoref{fig:cyclicvector}.

\begin{figure}
  \begin{center}
    \includegraphics[width=12cm]{splitvector}
  \end{center}
  \caption{
    Illustration of cyclic partition of a vector. Each process writes a single
    number, followed by a gap of $\text{size}-1$ numbers. This pattern repeats for the
    extent of the data.
  }
  \label{fig:cyclicvector}
\end{figure}

If we perform this I/O naively, each process would write a single number,
perform a seek, write another number and so on. Such small writes followed by
seeks makes it very hard for the system to optimize the I/O. MPI-IO offers a
much better approach. It builds on the MPI machinery for constructing custom
types. While these datatypes are used to describe the data layout in memory in
standard MPI functions, they are here used to describe the data layout on
secondary storage. From each process, the data access pattern is one number
followed by a gap of $\text{size}-1$ numbers. We can describe such a pattern in
a MPI datatype as
\lstinputlisting[style=c]{createsplitview.c}
We here use the function
\begin{lstlisting}[style=c]
  MPI_Type_create_resized(origtype, lb, extent, newtype)
\end{lstlisting}
to create the gap. We set \texttt{lb} (lower bound) = 0 to indicate that we want
the real data to start at the beginning of the new data type. We then extend
this to be size doubles long. We can now attach this view of the data to the
file using the function
\begin{lstlisting}[style=c]
  MPI_File_set_view(fh, disp, datatype,
                    viewtype, encoding, info)
\end{lstlisting}
as
\lstinputlisting[style=c]{setsplitview.c}
Note the usage of the \emph{disp} field. This field describes an offset that the
process sees as the start of the file. By setting this offset equal to position
of the first number the particular process should write to the file, the
data access pattern (\emph{MPI\_Datatype}) we described further up is exactly
the same on each process. With this configured, writing the data to the file in
the proper pattern is now just a normal write call, e.g. using separate handles
\lstinputlisting[language=C]{writesplitview.c}
A code demonstrating this can be found in Appendix \ref{app:cyclicvector}.

We stress that there is nothing stopping you from using different fileviews on
the different processes. In fact, we now move on to consider a particular case
where this is needed, namely when we want to handle arrays/matrices partitioned
over several processes.

\begin{figure}
  \begin{subfigure}{6cm}
    \includegraphics[width=6cm]{strip}
    \caption{Strips.}
    \label{fig:strip}
  \end{subfigure}
  \begin{subfigure}{6cm}
    \includegraphics[width=6cm]{block}
    \caption{Blocks.}
    \label{fig:block}
  \end{subfigure}
  \caption{
    Illustration of two possible ways a 2D array can be distributed across
    several processes. In \autoref{fig:strip} each process is responsible for a
    number of whole columns. In \autoref{fig:block} each process is instead
    reponsible for a sub-block of the array.
  }
  \label{fig:darrays}
\end{figure}

\section{Non-sequential I/O: distributed arrays}

In HPC applications, your data usually consists of matrices (2D arrays) or 3D
arrays. This fact has not gone past the MPI creators, and as such MPI contains
machinery for generating partioning of such arrays in a semi-automatic way.
Since MPI-IO is built on MPI, this machinery is also extremely useful when we
want to write the data to secondary storage. We now show how to employ these
utility functions to make writing distributed arrays to disc as simple as
performing a single write call.

MPI names such distributed arrays \emph{darray} for short. The available
functions are heavily inspired by the High Performance Fortran standard, HPF for
short. This is a standard which aims to make developing HPC software easier
through semi-automatic parallelization. In this context, the most important
parts of this standard is its conventions for array partitioning strategies and
process topologies, since MPI follows the same conventions. The two possible
partitionings of most interest in this context is illustrated in
\autoref{fig:darrays}. As we will show shortly, these are to some extent
similar.

\begin{figure}
  \begin{center}
    \includegraphics[width=12cm]{splitdomain}
  \end{center}
  \caption{
    A block partition of a 2D array with the pieces of information we need to
    classify this partitioning.
  }
  \label{fig:splitdomain}
\end{figure}

We now show how to use the MPI machinery to easily partition an array. Consider
\autoref{fig:splitdomain}. We here have a 2D array we want to partition across 6
MPI processes. The figure contains the pieces of information we need to classify
a partitioning:
\begin{itemize}
\item A global topology: here a Cartesian topology expressed as the number
  of processes along each dimension (here 2 and 3, respectively).
\item Location of a particular domain in the topology, again
  this can be expressed as an integer along each dimension.
\item A mapping of the available processes onto the topology.
\end{itemize}

The first useful function is \texttt{MPI\_Dims\_create}. This function generates
a Cartesian partioning of your processes according to a the rules set by
HPF.
\lstinputlisting[style=c]{dimsblock.c}
The initialization of the entries in sizes prior to calling the function is
important. In particular, the function will only operate along a dimension $i$
where \texttt{sizes[i] = 0} on function entry. While this interface makes it
very easy for the programmer to make errors (i.e. forgetting to initialize the
array properly), it also has advantages. For instance, if we instead want to
divide our matrix in a strip fashion, we can do
\lstinputlisting[style=c]{dimsstrip.c}
Of course, if we only have one partitioned dimension in our array, generating
the topology is trivial. The nice thing with doing it like this is that it
allows us to use fairly similar code, whether we want a strip or a block
partitioning of the arrays; see \autoref{fig:darrays}. We stress that this only
generates the \emph{topology} (or partitioning structure), it does not in any
way depend on the array dimensions. Upon return from the function the
\texttt{dims} array contains the number of processes used in the separate
directions, 2 and 3, respectively, in our example.

Now we have a topology describing the layout of our processors, or rather how
many processors the dimensions are split over. Still, each processor needs to
know where they are located in this topology. To be able to decide this, we
first have to create a communicator group which has the (Cartesian) topology
attached. This can be achieved by
\lstinputlisting[style=c]{comm.c}
The \texttt{periodic} array is an integer array with either 0 or 1 as entries.
These are used to specify whether or not the domain is periodic in the
particular dimension, which is not the case with a standard 2D array as the one
we consider here. Upon return from the function, the \texttt{comm} variable
holds the new communicator info. This can now be used wherever MPI expects a
communicator (i.e. instead of the builtin \texttt{MPI\_COMM\_WORLD} we have used
previously). This takes care of mapping the processes onto the topology.

Finally, each process can then find their location in the topology using
\lstinputlisting[style=c]{coord.c}
Upon return from the function, the \texttt{coords} array holds the coordinates.
In our example, it would for instance hold 1 and 2 when called on process 5.

With the problem partitioning taken care of, it is time to tie this into MPI-IO.
MPI includes functions to describe such a distributed array layout in memory,
which usually are useful when you want to collect a whole array on a single
process. Collecting all data on a single process is exactly what we want to
perform when we write this to secondary storage, the only difference is that in
our case this ``process'' is the file on secondary storage. Thus we can use the
functions originally intended for describing the data layout in memory to
instead generate the fileviews on the separate processes. The function we need
is
\begin{lstlisting}[style=c]
  MPI_Type_create_darray(size, rank, dims, gsizes,
                         distribs, dargs, sizes,
                         order, etype, newtype)
\end{lstlisting}
Here \texttt{size} is the size of the communicator (typically the number of
processes), \texttt{rank} the process rank within the communicator,
\texttt{dims} the number of dimensions in the array (2 for a matrix),
\texttt{gsizes} the sizes of the global array along the dimensions,
\texttt{distribs} distribution strategies along the dimensions, \texttt{dargs} a
distribution strategy parameter (can usually be set to
\texttt{MPI\_DISTRIBUTE\_DFLT\_DARG}), \texttt{dims} the topology results as
obtained using \texttt{MPI\_Dims\_create}, \texttt{order} describes the array
layout in memory (\texttt{MPI\_ORDER\_C} or \texttt{MPI\_ORDER\_FORTRAN}),
\texttt{etype} the datatype of the array entries and finally \texttt{newtype}
our new datatype.

The most interesting of these are the \texttt{distribs} array. This array
contains the chosen distribution strategy along each dimension. The strategy can
be one of
\begin{itemize}
\item \texttt{MPI\_DISTRIBUTE\_NONE}: Here no partitioning of the array is applied along
  this dimension.
\item \texttt{MPI\_DISTRIBUTE\_CYCLIC}: Here a cyclic partitioning of the array is applied
  along this dimension. This distribution is often used in HPF codes. We do not
  consider it here.
\item \texttt{MPI\_DISTRIBUTE\_BLOCK}: Here a block partitioning of the array is applied
  along this dimension.
\end{itemize}
For instance, if we want to block-partition an array of doubles with size
$N\times N$ stored according to C convections (row-major storage), we do
\lstinputlisting[style=c]{createarrayview.c}
If we now set this datatype as the fileview on the individual processes as
described earlier, we can now write them to disc using a single write call. This
is much much easier than the alternative using separate write/seek calls.

A code that handles both strip and block partitioned arrays can be found in
Appendix \ref{app:darray}.

\section{Overlapping I/O and computations}

On modern architectures HDDs can write/read data (almost) completely on their
own using a technique known as \emph{Direct Memory Access}, \emph{DMA} for
short. This means that while we are writing/reading data the CPU is mostly an
idle observer. This is not good for program efficiency, ideally we would like to
keep the CPUs saturated with work whenever we can. MPI-IO also offers facilities
to remedy this, namely \emph{non-blocking} I/O. All classes of I/O calls have
non-blocking equivalents, to simplify the presentation we here only consider
using individual handles on each process.

The basic idea can be summarized in the following piece of (somewhat) abstract
code;
\lstinputlisting[style=c]{overlapping.c}
We have here replaced the \texttt{MPI\_File\_write} function with a call to
\texttt{MPI\_File\_iwrite}, which is the nonblocking equivalent. This call
initiates the I/O operation, then immediately returns. The function takes an
additional parameter of type \texttt{MPI\_Request}. Upon the return from the
function call, this will be updated with information which identifies the I/O
operation. We are now free to perform additional calculations, as long as the
data we just requested written to secondary storage is not touched by this code.
In particular, the vector \texttt{vec} cannot be updated in the
\texttt{doSomething()} call. When we get to a point where we need write access
to the vector \texttt{vec} again, we do a call to \texttt{MPI\_Wait} with the
\texttt{MPI\_Request} variable as a parameter. This function will only return
once it is safe to reuse \texttt{vec}. Note that a call to a nonblocking I/O
function \emph{ALWAYS} needs to be accompanied with a call to
\texttt{MPI\_Wait}, even if you do not plan to reuse the memory area you
requested written to secondary storage.

\section{Tuning for performance}

As mentioned in the introduction, a program using MPI-IO needs to be tuned to
the particular underlying filesystem if we want the best performance. This
tuning can be divided in two classes.

\begin{itemize}
\item \textbf{Directives}: This class of tuning parameters are something an
  implementation has to obey. One example of such a tuning parameter is the flag
  \texttt{MPI\_MODE\_SEQUENTIAL} which can be passed upon opening the file, just
  like we passed e.g. \texttt{MPI\_MODE\_WRONLY} earlier. This tells the system
  that only sequential access to the data is performed, information which can be
  used to optimze the I/O.
\item \textbf{Hints}: These are, as the name indicates, just hints to the
  implementation. If an implementation does not support/utilize a particular
  hint, they can just be silently ignored. This is the framework in which vendor
  specific/filesystem specific tuning can be performed.
\end{itemize}

These hints are passed to the implementation in a variable of type
\texttt{MPI\_Info}. First we need to create the appropriate structure, this is
achieved through
\lstinputlisting[style=c]{infoinit.c}
We can now set hints in this structure. A hint is a pair of (key,value) strings.
We add such a hint to our \texttt{MPI\_Info} variable using the function
\begin{lstlisting}[style=c]
  MPI_Info_set(info, key, value)
\end{lstlisting}
Such a call might look like
\lstinputlisting[style=c]{infoset.c}

Once we have added all hints we want to the \texttt{MPI\_Info} variable, we can
now pass these hints to the implementation upon opening a file.
\lstinputlisting[style=c]{infoopen.c}
We here pass our \texttt{MPI\_Info} variable instead of \texttt{MPI\_INFO\_NULL}
as earlier.

\section{Further reading}
You can find the MPI-IO standard as well as tutorials on the official
MPI homepage \url{http://www.mpi-forum.org/}. Some useful papers and slides
can be found at
\url{https://computing.llnl.gov/?set=code\&page=sio\_papers\_presentations}.
These are of particular interest if you want extensive knowledge about using
MPI-IO on top of GPFS.

\textbf{Acknowledgements}: J{\o}rn Amundsen gave some useful pointers while this
chapter was written, in particular the link to \url{llnl.gov}. Tobias Arrskog
helped with proof reading. The chapter was written by Arne Morten Kvarving. Your
assistance was greatly appreciated.
